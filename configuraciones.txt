Buena vibra con sent: 
#Inicializar el modelo Llama2
        self.llm = Llama(
            model_path=self.model_path,
            verbose=True,
            n_gpu_layers=14,
            n_ctx=4096,
            **self.device_options,  # Usa las opciones correspondientes según el sistema operativo
            chat_format="llama-2",
            temp = 0.810000,
            top_k = 80, 
            ktop_p = 0.950000,
            use_mlock=False,
            rope_scaling_type = 0,  # Puedes probar con diferentes valores según tu tarea
            rope_freq_base = 0,     # Puedes ajustar según tus necesidades o mantenerlo desde el modelo
            rope_freq_scale = 0,    # Puedes ajustar según tus necesidades o mantenerlo desde el model
            numa=False,
            max_length=3500,
        )

        # Historial de conversación
        self.conversation_history = []
        self.mensaje_sistema ='Éres un asistente de programación que solo sabe hablar en español'
        self.mensaje_sistema2 = "Éres muy maleducado y sarcástico"

        self.conversation_history.append({"role": "system", "content": self.mensaje_sistema})
        self.conversation_history.append({"role": "system", "content": self.mensaje_sistema2})

    def add_user_input(self, user_input):
        # Añadir input del usuario al historial de la conversación
        self.conversation_history.append({"role": "user", "content": user_input})

    def get_assistant_response(self):
        last_user_input_time = time.time()

        # Realizar la inferencia
        output = self.llm.create_chat_completion(messages=self.conversation_history,
                                                max_tokens=8192,repeat_penalty=1,frequency_penalty=1,presence_penalty=1.3)
                                                 # Puedes ajustar según tus necesidades o mantenerlo desde el modelo



Normal:
        # Inicializar el modelo Llama2
        self.llm = Llama(
            model_path=self.model_path,
            verbose=True,
            n_gpu_layers=14,
            n_ctx=4096,
            **self.device_options,  # Usa las opciones correspondientes según el sistema operativo
            chat_format="llama-2",
            temp = 0.810000,
            top_k = 80, 
            ktop_p = 0.950000,
            use_mlock=False,
            rope_scaling_type = 0,  # Puedes probar con diferentes valores según tu tarea
            rope_freq_base = 0,  # Puedes ajustar según tus necesidades o mantenerlo desde el modelo
            rope_freq_scale = 0,  # Puedes ajustar según tus necesidades o mantenerlo desde el model
            numa=False,
            max_length=3500,
        )

        # Historial de conversación
        self.conversation_history = []
     
        self.conversation_history.append({"role": "system", "content": self.mensaje_sistema})
        self.conversation_history.append({"role": "system", "content": self.mensaje_sistema2})



    def add_user_input(self, user_input):
        # Añadir input del usuario al historial de la conversación
        self.conversation_history.append({"role": "user", "content": user_input})


    def get_assistant_response(self):
        last_user_input_time = time.time()

        # Realizar la inferencia
        output = self.llm.create_chat_completion(messages=self.conversation_history,
                                                    max_tokens=256,
                                                    repeat_penalty=1,
                                                    frequency_penalty=1,
                                                    presence_penalty=1.3)